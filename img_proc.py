# -*- coding: utf-8 -*-
"""Img_proc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_S4YaakzRE0CXn6sOdAF9PU_jjP37ATT
"""
import cv2
import numpy as np
import random
from torchvision import transforms
from skimage import img_as_float, img_as_ubyte
import torch
import torchvision.transforms.functional as Func
import albumentations as A
from albumentations.pytorch import ToTensorV2


def get_image_path(image_id):
    if int(image_id[7:]) < 29306: #HAM10000 part 1
      image_path = f"/root/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2/HAM10000_images_part_1/{image_id}.jpg"
    else: #HAM10000 part 2
      image_path = f"/root/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2/HAM10000_images_part_2/{image_id}.jpg"
    return image_path

def get_image_path_bcn(image_id):
    image_path = f'/root/.cache/kagglehub/datasets/pasutchien/bcn20000/versions/1/BCN_20k_train/bcn_20k_train/{image_id}'
    return image_path
def get_image_path_pad(image_id):
    image_path = f'/root/.cache/kagglehub/datasets/pasutchien/pad-ufes-20-2/versions/1/zr7vgbcyr2-1/images/images/images/{image_id}'
    return image_path
def shades_of_gray_skimage(image, p=6):
    image = img_as_float(image)

    L = np.power(np.sum(np.abs(image) ** p, axis=(0, 1)), 1 / p)

    normalized_image = image / L

    normalized_image /= normalized_image.max()
    return img_as_ubyte(normalized_image)

def remove_border(image_gray, image_rgb): #Done only on BCN
    '''
    As Images in BCN20000 are not processed, we will do some image processing and 
    try to produce images that have the same quality as the HAM10000 images. Luckily,
    CNNs are invariant to many kinds of noises; 
    however, black borders do not work with kernels because it isn't a part of the image, 
    so we will remove it.
    '''
    #Receives grayscale and RGB of that image as input
    _, binary_image = cv2.threshold(image_gray, 50, 255, cv2.THRESH_BINARY)

    height, width = binary_image.shape
    mid_height = height // 2  # 512
    mid_width = width // 2    # 512
    old_area = height*width

    #Left border
    l_b = 0
    for i in range(width):
        if binary_image[mid_height, i] != 0:
            l_b = i
            break

    #top_border
    t_b = 0
    for i in range(height):
        if binary_image[i, mid_width] != 0:
            t_b = i
            break

    cropped_image_color = image_rgb[t_b:1024-t_b, l_b:1024-l_b]

    new_height, new_width = cropped_image_color.shape[:2]
    new_area = new_height*new_width
    if new_area/old_area > 0.9:
        cropped_image_color = image_rgb #Don't Crop

    return cropped_image_color

def process_image(image, width, height, validation):
    '''
    As Image in HAM10000 is fairly preprocessed and Image from BCN20000 has its border removed
    , we will do minor adjustments to the image which are resizing and center cropping.
    '''
    #Step 1: Resizing image so that short side is 1.25* larger than input size
    new_height = 1.25* height
    old_height, old_width = image.shape[:2]
    aspect_ratio = old_width/old_height
    new_width = new_height*aspect_ratio

    image = cv2.resize(image,(int(new_width), int(new_height)))

    #Step2: Random Square Center Crop with size between [0.8,1] of the resized image
    if validation == False:
      relative_size = np.random.uniform(0.8, 1)
    else:
      relative_size = 0.9 #Fixed size of 0.9 for validation and test set
    transform = transforms.Compose([
        transforms.ToTensor(),  # Convert to tensor and scale to [0, 1]
        transforms.CenterCrop((int(new_height * relative_size), int(new_width * relative_size)))
    ])
    cropped = transform(image)


    #Step3: Resize cropped image to the desired input size
    resize = transforms.Resize((height, width))
    return resize(cropped)
def get_image(image_id, for_CNN=True, width=24, height=18,  validation = False):
    #A function for getting image from our meta data
    if image_id.split("_")[0] == "BCN":
        image_path = get_image_path_bcn(image_id)
    elif image_id.split("_")[0] == "PAT":
        image_path = get_image_path_pad(image_id)
    else:
        image_path = get_image_path(image_id)
    try:
        image = cv2.imread(image_path, cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #Convert BGR to RGB
        #For CNN, we preprocess and Normalize + Augmentation for train set
        if for_CNN == True:
          if image_id.split("_")[0] == "BCN": #For BCN, we remove border
              image_gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
              image = remove_border(image_gray, image)
          image = shades_of_gray_skimage(image)
          image = process_image(image, width, height, validation)
          transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
          image = transform(image)
          if validation == False: #Apply augmentation for training set
              image = np.transpose(image, [1,2,0])
              image = (image / 2 + 0.5).numpy()
              image = (image * 255).astype(np.uint8)
              image = augment_image(image, width)[0]
        #For Decision Tree and Logistic Regression, we resize and flatten
        else:
          image = cv2.resize(image,(width,height))
          image = image.flatten()
        return image
    except FileNotFoundError:
        print(f"Image {image_id} not found.")
        return None


def create_transform_pipeline(img_size=224):

    return A.Compose([
        A.Transpose(p=0.5),
        A.VerticalFlip(p=0.5),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightness(limit=0.2, p=0.75),
        A.RandomContrast(limit=0.2, p=0.75),
        A.OneOf([
            A.MotionBlur(blur_limit=5),
            A.MedianBlur(blur_limit=5),
            A.GaussianBlur(blur_limit=5),
            A.GaussNoise(var_limit=(5.0, 30.0)),
        ], p=0.7),

        A.OneOf([
            A.OpticalDistortion(distort_limit=1.0),
            A.GridDistortion(num_steps=5, distort_limit=1.),
            A.ElasticTransform(alpha=3),
        ], p=0.7),

        A.CLAHE(clip_limit=4.0, p=0.7),
        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),
        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),
        A.Resize(img_size, img_size),
        A.Cutout(max_h_size=int(img_size * 0.375), max_w_size=int(img_size * 0.375), num_holes=1, p=0.7),
        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ToTensorV2()
    ])


def augment_image(image, img_size):

    transform = create_transform_pipeline(img_size)
    augmented = transform(image=image)
    return [augmented['image']]
